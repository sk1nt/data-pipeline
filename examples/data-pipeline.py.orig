#!/usr/bin/env python3
"""Local bridge server for capturing GEXBot XHR payloads.

This module implements a local HTTP server that acts as a bridge to capture
and persist GEX (Gamma Exposure) data payloads from GEXBot. It handles incoming
POST requests containing ticker snapshots, majors, maxchange, and snapshot data,
normalizes and stores them in an SQLite database, and provides endpoints for
queuing historical data imports.

Key components:
- GEXRequestHandler: HTTP request handler for /gex and /gex_history_url endpoints

- TickerSnapshot: Dataclass representing aggregated ticker data from multiple payloads


"""

import argparse
import json
import logging
import os
import sqlite3
import subprocess
import sys
import threading
import atexit
import time
from queue import Queue, Empty
from dataclasses import dataclass, field
from datetime import datetime
from http import HTTPStatus
from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple, Union
from market_ml.depth import persist_raw_event
from market_ml.webhook_schemas import validate_webhook_payload, validate_gex_payload
from market_ml.retry_utils import retry_db_operation, RetryableQueue
from market_ml.gex_snapshot import (
    write_snapshot_atomic,
    GEXSnapshot,
    ChangeWindow,
    normalize_ticker as gex_normalize_ticker,
)
from market_ml.gexbot import load_historical_gex
from market_ml.config import config
import subprocess
import requests
from dotenv import load_dotenv
import asyncio


# Load environment variables
load_dotenv()

LOG = logging.getLogger("webhook")


# Legacy aliases are no longer needed - all systems now use canonical format
# NQ_NDX for Nasdaq futures, ES_SPX for S&P futures
TICKER_ALIAS: Dict[str, str] = {}

# Trading configuration
#PROJECT_ROOT = Path(__file__).resolve().parent
#OUTPUT_DIR = PROJECT_ROOT / "outputs" / "data" / "raw" / "gex"
#OUTPUT_FILE = OUTPUT_DIR / "latest.json"
#BRIDGE_DB_DIR = OUTPUT_DIR
#BRIDGE_DB_PATH = BRIDGE_DB_DIR / "gex_history.db"
#DB_LOCK = threading.Lock()
#HISTORY_LABELS = ["1m", "5m", "10m", "15m", "30m"]
#HISTORY_QUEUE_TABLE = "gex_history_queue"


#def _get_pk_columns(conn: sqlite3.Connection, table: str) -> list[str]:
##    """Get primary key column names for a table in sorted order."""
#    pk_columns = []
#    for row in conn.execute(f"PRAGMA table_info({table})"):
#        pk_order = row[5]
###        if pk_order:
#            pk_columns.append((pk_order, row[1]))
# #   pk_columns.sort()
#    return [name for _, name in pk_columns]


def _migrate_bridge_schema(conn: sqlite3.Connection) -> None:
    """Migrate database schema to include endpoint in primary keys if needed."""
    pk_snapshots = _get_pk_columns(conn, "gex_bridge_snapshots")
    if pk_snapshots != ["ticker", "timestamp", "endpoint"]:
        LOG.info("Migrating gex_bridge_snapshots to include endpoint in primary key")
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS gex_bridge_snapshots_new (
                ticker TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                endpoint TEXT NOT NULL,
                payload_kind TEXT,
                received_at TEXT,
                spot REAL,
                zero_gamma REAL,
                net_gex_vol REAL,
                net_gex_oi REAL,
                major_pos_vol REAL,
                major_neg_vol REAL,
                major_pos_oi REAL,
                major_neg_oi REAL,
                delta_risk_reversal REAL,
                strikes_count INTEGER,
                max_change_json TEXT,
                max_priors_json TEXT,
                raw_payload_json TEXT,
                updated_at TEXT,
                PRIMARY KEY (ticker, timestamp, endpoint)
            )
            """
        )
        conn.execute(
            """
            INSERT OR REPLACE INTO gex_bridge_snapshots_new (
                ticker, timestamp, endpoint, payload_kind, received_at,
                spot, zero_gamma, net_gex_vol, net_gex_oi,
                major_pos_vol, major_neg_vol, major_pos_oi, major_neg_oi,
                delta_risk_reversal, strikes_count, max_change_json,
                max_priors_json, raw_payload_json, updated_at
            )
            SELECT
                ticker,
                timestamp,
                CASE
                    WHEN endpoint IS NULL OR endpoint = '' THEN 'unknown'
                    ELSE endpoint
                END AS endpoint,
                payload_kind,
                received_at,
                spot,
                zero_gamma,
                net_gex_vol,
                net_gex_oi,
                major_pos_vol,
                major_neg_vol,
                major_pos_oi,
                major_neg_oi,
                delta_risk_reversal,
                strikes_count,
                max_change_json,
                max_priors_json,
                raw_payload_json,
                updated_at
            FROM gex_bridge_snapshots
            """
        )
        conn.execute("DROP TABLE gex_bridge_snapshots")
        conn.execute("ALTER TABLE gex_bridge_snapshots_new RENAME TO gex_bridge_snapshots")
        conn.execute("DROP INDEX IF EXISTS idx_bridge_snapshots_ts")
        conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_bridge_snapshots_ts ON gex_bridge_snapshots(timestamp, endpoint)"
        )

    pk_strikes = _get_pk_columns(conn, "gex_bridge_strikes")
    if pk_strikes != ["ticker", "timestamp", "endpoint", "strike"]:
        LOG.info("Migrating gex_bridge_strikes to include endpoint in primary key")
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS gex_bridge_strikes_new (
                ticker TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                endpoint TEXT NOT NULL,
                strike REAL NOT NULL,
                gamma_now REAL,
                vanna REAL,
                gamma_1m REAL,
                gamma_5m REAL,
                gamma_10m REAL,
                gamma_15m REAL,
                gamma_30m REAL,
                history_json TEXT,
                PRIMARY KEY (ticker, timestamp, endpoint, strike)
            )
            """
        )
        conn.execute(
            """
            INSERT OR REPLACE INTO gex_bridge_strikes_new (
                ticker, timestamp, endpoint, strike,
                gamma_now, vanna, gamma_1m, gamma_5m, gamma_10m,
                gamma_15m, gamma_30m, history_json
            )
            SELECT
                gs.ticker,
                gs.timestamp,
                CASE
                    WHEN snap.endpoint IS NULL OR snap.endpoint = '' THEN 'unknown'
                    ELSE snap.endpoint
                END AS endpoint,
                gs.strike,
                gs.gamma_now,
                gs.vanna,
                gs.gamma_1m,
                gs.gamma_5m,
                gs.gamma_10m,
                gs.gamma_15m,
                gs.gamma_30m,
                gs.history_json
            FROM gex_bridge_strikes AS gs
            LEFT JOIN gex_bridge_snapshots AS snap
                ON snap.ticker = gs.ticker
                AND snap.timestamp = gs.timestamp
            """
        )
        conn.execute("DROP TABLE gex_bridge_strikes")
        conn.execute("ALTER TABLE gex_bridge_strikes_new RENAME TO gex_bridge_strikes")
        conn.execute("DROP INDEX IF EXISTS idx_bridge_strikes_ts")
        conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_bridge_strikes_ts ON gex_bridge_strikes(timestamp, endpoint)"
        )


def _ensure_bridge_db() -> bool:
    """Initialize the bridge database with required tables and indexes."""
    try:
        BRIDGE_DB_DIR.mkdir(parents=True, exist_ok=True)
        with sqlite3.connect(BRIDGE_DB_PATH, timeout=30) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS gex_bridge_snapshots (
                    ticker TEXT NOT NULL,
                    timestamp INTEGER NOT NULL,
                    endpoint TEXT NOT NULL,
                    payload_kind TEXT,
                    received_at TEXT,
                    spot REAL,
                    zero_gamma REAL,
                    net_gex_vol REAL,
                    net_gex_oi REAL,
                    major_pos_vol REAL,
                    major_neg_vol REAL,
                    major_pos_oi REAL,
                    major_neg_oi REAL,
                    delta_risk_reversal REAL,
                    strikes_count INTEGER,
                    max_change_json TEXT,
                    max_priors_json TEXT,
                    raw_payload_json TEXT,
                    updated_at TEXT,
                    PRIMARY KEY (ticker, timestamp, endpoint)
                )
                """
            )
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS gex_bridge_strikes (
                    ticker TEXT NOT NULL,
                    timestamp INTEGER NOT NULL,
                    endpoint TEXT NOT NULL,
                    strike REAL NOT NULL,
                    gamma_now REAL,
                    vanna REAL,
                    gamma_1m REAL,
                    gamma_5m REAL,
                    gamma_10m REAL,
                    gamma_15m REAL,
                    gamma_30m REAL,
                    history_json TEXT,
                    PRIMARY KEY (ticker, timestamp, endpoint, strike)
                )
                """
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_bridge_strikes_ts ON gex_bridge_strikes(timestamp, endpoint)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_bridge_snapshots_ts ON gex_bridge_snapshots(timestamp, endpoint)"
            )
            conn.execute(
                f"""
                CREATE TABLE IF NOT EXISTS {HISTORY_QUEUE_TABLE} (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    url TEXT NOT NULL,
                    ticker TEXT NOT NULL,
                    endpoint TEXT NOT NULL,
                    status TEXT NOT NULL DEFAULT 'pending',
                    attempts INTEGER NOT NULL DEFAULT 0,
                    last_error TEXT,
                    created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            conn.execute(
                f"CREATE INDEX IF NOT EXISTS idx_history_queue_status ON {HISTORY_QUEUE_TABLE}(status, created_at)"
            )
            _migrate_bridge_schema(conn)
        return True
    except Exception:
        LOG.exception("Failed to initialize bridge database at %s", BRIDGE_DB_PATH)
        return False


BRIDGE_DB_READY = _ensure_bridge_db()


def _normalize_timestamp(value: Union[int, float, str, None]) -> Optional[int]:
    """Normalize timestamp value to integer seconds since epoch."""
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return int(value)
    if isinstance(value, str):
        try:
            return int(float(value))
        except ValueError:
            try:
                dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
                return int(dt.timestamp())
            except ValueError:
                return None
    return None


def _json_dump(data: object) -> str:
    """Serialize data to JSON string, falling back to string representation on failure."""
    try:
        return json.dumps(data, ensure_ascii=False)
    except TypeError:
        return json.dumps(str(data))


def _prepare_strike_rows(
    ticker: str,
    timestamp_value: int,
    endpoint_value: str,
    strikes: Sequence[Sequence[object]],
) -> Tuple[int, List[Tuple[object, ...]]]:
    """Prepare strike data rows for database insertion."""
    rows = []
    for entry in strikes:
        if not isinstance(entry, (list, tuple)) or len(entry) < 2:
            continue
        strike_price = entry[0]
        gamma_now = entry[1]
        vanna_now = entry[2] if len(entry) > 2 else None
        history = entry[3] if len(entry) > 3 and isinstance(entry[3], (list, tuple)) else []
        history_values = []
        for idx, label in enumerate(HISTORY_LABELS):
            history_values.append(history[idx] if len(history) > idx else None)
        rows.append(
            (
                ticker,
                timestamp_value,
                endpoint_value,
                strike_price,
                gamma_now,
                vanna_now,
                history_values[0],
                history_values[1],
                history_values[2],
                history_values[3],
                history_values[4],
                _json_dump(history) if history else None,
            )
        )
    return len(rows), rows

def get_gex_refresh_interval():
    """
    Returns refresh interval (seconds) based on current time and market hours.
    Afterhours (6pm-8:15pm): 300s (5 min)
    Pre-market (8:15pm-9:30am): 60s (1 min)
    RTH (9:30am-4:00pm): 1s (1 sec)
    """
    now = datetime.now()
    hour = now.hour
    minute = now.minute
    # Afterhours: 18:00-20:15
    if (hour == 18 and minute >= 0) or (18 < hour < 20) or (hour == 20 and minute <= 15):
        return 300
    # Pre-market: 20:15-9:30
    if (hour == 20 and minute > 15) or (21 <= hour <= 23) or (0 <= hour < 9) or (hour == 9 and minute < 30):
        return 60
    # RTH: 9:30-16:00
    if (hour == 9 and minute >= 30) or (10 <= hour < 16):
        return 1
    # Default
    return 60

def fetch_gex_api_and_save_json(api_func, ticker, json_path):
    """
    Periodically fetch GEX API data and save to a local JSON file with dynamic interval.
    Args:
        api_func: function to call for GEX API (should return dict)
        ticker: ticker symbol
        json_path: path to save JSON file
    """
    def _worker():
        while True:
            try:
                data = api_func(ticker)
                data['timestamp'] = datetime.now().isoformat()
                with open(json_path, 'w') as f:
                    json.dump(data, f)
                print(f"[GEX Bridge] Refreshed {ticker} GEX data to {json_path}")
            except Exception as e:
                print(f"[GEX Bridge] API refresh failed: {e}")
            interval_sec = get_gex_refresh_interval()
            time.sleep(interval_sec)
    thread = threading.Thread(target=_worker, daemon=True)
    thread.start()

def start_gex_api_refresh(api_func, symbols, out_dir="outputs/gex"):
    """
    Start periodic API refresh for each symbol, saving to local JSON files with dynamic interval.
    Args:
        api_func: function to call for GEX API (should return dict)
        symbols: list of ticker symbols
        out_dir: output directory for JSON files
    """
    os.makedirs(out_dir, exist_ok=True)
    for symbol in symbols:
        json_path = os.path.join(out_dir, f"{symbol.lower()}_gex_status.json")
        fetch_gex_api_and_save_json(api_func, symbol, json_path)

@retry_db_operation
def persist_snapshot_to_db(
    ticker: str,
    snapshot: "TickerSnapshot",
    payload_kind: str,
    raw_payload: dict,
    endpoint: Optional[str],
) -> None:
    """Persist a ticker snapshot to the database with automatic retry."""
    if not BRIDGE_DB_READY:
        return

    snapshot_dict = snapshot.to_dict()
    combined = snapshot_dict.get("combined", {})
    timestamp_value = _normalize_timestamp(combined.get("timestamp"))
    if timestamp_value is None:
        LOG.debug("Skipping DB persist for %s: missing timestamp", ticker)
        return

    endpoint_value = endpoint or snapshot.last_endpoint or "unknown"
    strikes_source = snapshot.snapshot.get("strikes") if snapshot.snapshot else None
    strikes_list = strikes_source if isinstance(strikes_source, Sequence) else []
    strikes_count, strike_rows = _prepare_strike_rows(ticker, timestamp_value, endpoint_value, strikes_list)

    snapshot_row = (
        ticker,
        timestamp_value,
        endpoint_value,
        payload_kind,
        snapshot.last_received_at,
        combined.get("spot"),
        combined.get("zero_gamma"),
        combined.get("net_gex_vol"),
        combined.get("net_gex_oi"),
        combined.get("major_pos_vol"),
        combined.get("major_neg_vol"),
        combined.get("major_pos_oi"),
        combined.get("major_neg_oi"),
        combined.get("delta_risk_reversal"),
        strikes_count,
        _json_dump(combined.get("max_change") or {}),
        _json_dump(combined.get("max_priors") or []),
        _json_dump(raw_payload),
        snapshot_dict.get("updated_at"),
    )

    try:
        with DB_LOCK, sqlite3.connect(BRIDGE_DB_PATH, timeout=30) as conn:
            conn.execute(
                """
                INSERT INTO gex_bridge_snapshots (
                    ticker, timestamp, endpoint, payload_kind, received_at,
                    spot, zero_gamma, net_gex_vol, net_gex_oi,
                    major_pos_vol, major_neg_vol, major_pos_oi, major_neg_oi,
                    delta_risk_reversal, strikes_count, max_change_json,
                    max_priors_json, raw_payload_json, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(ticker, timestamp, endpoint) DO UPDATE SET
                    endpoint=excluded.endpoint,
                    payload_kind=excluded.payload_kind,
                    received_at=excluded.received_at,
                    spot=excluded.spot,
                    zero_gamma=excluded.zero_gamma,
                    net_gex_vol=excluded.net_gex_vol,
                    net_gex_oi=excluded.net_gex_oi,
                    major_pos_vol=excluded.major_pos_vol,
                    major_neg_vol=excluded.major_neg_vol,
                    major_pos_oi=excluded.major_pos_oi,
                    major_neg_oi=excluded.major_neg_oi,
                    delta_risk_reversal=excluded.delta_risk_reversal,
                    strikes_count=excluded.strikes_count,
                    max_change_json=excluded.max_change_json,
                    max_priors_json=excluded.max_priors_json,
                    raw_payload_json=excluded.raw_payload_json,
                    updated_at=excluded.updated_at
                """
            , snapshot_row)

            if strike_rows:
                conn.execute(
                    "DELETE FROM gex_bridge_strikes WHERE ticker = ? AND timestamp = ? AND endpoint = ?",
                    (ticker, timestamp_value, endpoint_value),
                )
                conn.executemany(
                    """
                    INSERT INTO gex_bridge_strikes (
                        ticker, timestamp, endpoint, strike, gamma_now, vanna,
                        gamma_1m, gamma_5m, gamma_10m, gamma_15m, gamma_30m,
                        history_json
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    strike_rows,
                )
            conn.commit()
    except Exception:
        LOG.exception("Failed to persist snapshot for %s", ticker)


def enqueue_history_request(url: str, ticker: str, endpoint: str) -> int:
    """Enqueue a history import request in the database."""
    if not BRIDGE_DB_READY:
        raise RuntimeError("Bridge database is not initialized")
    clean_endpoint = endpoint or "unknown"
    with DB_LOCK, sqlite3.connect(BRIDGE_DB_PATH, timeout=30) as conn:
        cursor = conn.execute(
            f"""
            INSERT INTO {HISTORY_QUEUE_TABLE} (url, ticker, endpoint, status)
            VALUES (?, ?, ?, 'pending')
            """,
            (url, ticker, clean_endpoint),
        )
        conn.commit()
        queue_id = cursor.lastrowid

        # Trigger a background download/backfill job automatically.
        # Behavior can be configured with env var HISTORY_DOWNLOAD_COMMAND which may
        # contain placeholders: {id}, {url}, {ticker}, {endpoint}.
        try:
            cmd_template = os.getenv('HISTORY_DOWNLOAD_COMMAND')
            if cmd_template:
                cmd = cmd_template.format(id=queue_id, url=url, ticker=ticker, endpoint=clean_endpoint)
                LOG.info("Launching configured history download command: %s", cmd)
                # shell used intentionally to allow complex commands in env var
                subprocess.Popen(cmd, shell=True, cwd=str(PROJECT_ROOT))
            else:
                # Fallback: if the repository has scripts/backfill_gex_history.py, call it
                backfill_script = PROJECT_ROOT / 'scripts' / 'backfill_gex_history.py'
                if backfill_script.exists():
                    cmd = [sys.executable, str(backfill_script), '--db', str(BRIDGE_DB_PATH), '--limit-ticker', str(ticker)]
                    LOG.info("Launching backfill script: %s", ' '.join(cmd))
                    subprocess.Popen(cmd, cwd=str(PROJECT_ROOT))
                else:
                    LOG.debug("No HISTORY_DOWNLOAD_COMMAND configured and backfill script not found; skipping auto-download")
        except Exception:
            LOG.exception("Failed to launch history download job for queue id %s", queue_id)

        return queue_id


def append_spot_series(spot: float, timestamp) -> None:
    """Append raw spot/timestamp readings for quick diagnostics."""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    series_file = OUTPUT_DIR / "spot_series.jsonl"
    entry = {"timestamp": timestamp, "spot": spot}
    with series_file.open("a", encoding="utf-8") as handle:
        handle.write(json.dumps(entry) + "\n")


def normalise_ticker(ticker: str) -> str:
    """Normalize ticker name using predefined aliases."""
    ticker = (ticker or "").upper()
    return TICKER_ALIAS.get(ticker, ticker)


def write_consolidated_gex_snapshot(
    snapshot_obj: "TickerSnapshot",
    data: dict,
    payload_kind: str,
    endpoint: Optional[str]
) -> None:
    """
    Write GEX data to consolidated structure (FR-001, FR-002, FR-005-FR-007).
    
    Converts legacy TickerSnapshot format to new GEXSnapshot and persists
    to outputs/data/gex/{ticker}/ with snapshot.json, snapshot.csv, metadata.json.
    """
    from datetime import datetime, timezone
    
    # Get normalized ticker for directory naming
    normalized = gex_normalize_ticker(snapshot_obj.ticker)
    
    # Build combined view from snapshot object
    combined = snapshot_obj.to_dict().get("combined", {})
    
    # Extract timestamp
    timestamp_val = combined.get("timestamp")
    if timestamp_val is None:
        timestamp_val = data.get("timestamp")
    
    if isinstance(timestamp_val, (int, float)):
        # Unix timestamp to ISO format
        dt = datetime.fromtimestamp(timestamp_val, tz=timezone.utc)
        timestamp_iso = dt.isoformat()
    elif isinstance(timestamp_val, str):
        timestamp_iso = timestamp_val
    else:
        timestamp_iso = datetime.now(timezone.utc).isoformat()
    
    # Extract required fields with defaults
    spot_price = combined.get("spot") or 0.0
    zero_gamma = combined.get("zero_gamma") or 0.0
    net_gex = combined.get("net_gex_vol") or combined.get("net_gex_oi") or 0.0
    major_pos_strike = combined.get("major_pos_vol") or 0.0
    major_pos_vol = 0  # Volume not directly available in legacy format
    major_neg_strike = combined.get("major_neg_vol") or 0.0
    major_neg_vol = 0  # Volume not directly available in legacy format
    
    # Extract all strikes if available (FR-006)
    all_strikes = None
    strikes_source = snapshot_obj.snapshot.get("strikes") if snapshot_obj.snapshot else None
    if strikes_source and isinstance(strikes_source, list):
        all_strikes = []
        for entry in strikes_source:
            if isinstance(entry, (list, tuple)) and len(entry) >= 2:
                all_strikes.append({
                    "strike": float(entry[0]),
                    "gex": float(entry[1]),
                    "volume": 0,  # Not available in legacy format
                    "open_interest": 0,  # Not available in legacy format
                })
    
    # Extract change windows if available (FR-007)
    change_windows = None
    max_change = combined.get("max_change")
    if max_change and isinstance(max_change, dict):
        change_windows = []
        window_map = {
            "Now": "current",
            "1 min": "1m",
            "5 min": "5m",
            "10 min": "10m",
            "15 min": "15m",
            "30 min": "30m",
        }
        for label, window_key in window_map.items():
            if label in max_change:
                change_data = max_change[label]
                if isinstance(change_data, dict):
                    strike = change_data.get("strike", 0.0)
                    net_gex_val = change_data.get("net_gex", 0.0)
                    change_windows.append({
                        "window": window_key,
                        "strike": float(strike),
                        "old_gex": 0.0,  # Not available in this payload
                        "new_gex": float(net_gex_val),
                        "delta": float(net_gex_val),  # Approximation
                    })
    
    # Create GEXSnapshot
    gex_snapshot = GEXSnapshot(
        ticker=normalized,
        timestamp=timestamp_iso,
        spot_price=float(spot_price),
        zero_gamma=float(zero_gamma),
        net_gex=float(net_gex),
        major_pos_strike=float(major_pos_strike),
        major_pos_vol=int(major_pos_vol),
        major_neg_strike=float(major_neg_strike),
        major_neg_vol=int(major_neg_vol),
        all_strikes=all_strikes,
        change_windows=change_windows,
    )
    
    # Write atomically (FR-004)
    write_snapshot_atomic(gex_snapshot)
    LOG.debug(f"Wrote consolidated GEX snapshot for {normalized}")
        

@dataclass
class TickerSnapshot:
    """Represents aggregated ticker data from multiple GEX payloads.

    This dataclass holds the latest state of a ticker's GEX data, merging
    information from majors, maxchange, and snapshot payloads.
    """

    ticker: str
    majors: Optional[dict] = None
    maxchange: Optional[dict] = None
    snapshot: Optional[dict] = None
    updated_at: Optional[str] = None
    last_endpoint: Optional[str] = None
    last_payload_kind: Optional[str] = None
    last_received_at: Optional[str] = None

    def merge_payload(
        self,
        kind: str,
        payload: dict,
        received_at: Optional[str],
        context: Optional[dict] = None,
    ) -> None:
        """Merge a new payload into the snapshot."""
        if kind == "majors":
            self.majors = payload
        elif kind == "maxchange":
            self.maxchange = payload
        elif kind == "snapshot":
            self.snapshot = payload
        else:
            raise ValueError(f"Unsupported payload kind: {kind}")
        self.updated_at = received_at or self.updated_at
        self.last_received_at = received_at or self.last_received_at
        self.last_payload_kind = kind
        if context:
            endpoint = context.get("endpoint")
            if endpoint:
                self.last_endpoint = endpoint

    def to_dict(self) -> dict:
        """Convert the snapshot to a dictionary representation."""
        combined = build_combined_view(self.majors, self.maxchange, self.snapshot)
        if self.last_endpoint:
            combined["source_endpoint"] = self.last_endpoint
        combined["has_snapshot"] = self.snapshot is not None
        return {
            "ticker": self.ticker,
            "updated_at": self.updated_at,
            "majors": self.majors,
            "maxchange": self.maxchange,
            "snapshot": self.snapshot,
            "metadata": {
                "last_endpoint": self.last_endpoint,
                "last_payload_kind": self.last_payload_kind,
                "received_at": self.last_received_at,
            },
            "combined": combined,
        }


def build_combined_view(
    majors: Optional[dict],
    maxchange: Optional[dict],
    snapshot: Optional[dict],
) -> dict:
    """Combine majors and max change payloads into a simpler dictionary."""
    result: Dict[str, Optional[float]] = {
        "spot": None,
        "zero_gamma": None,
        "net_gex_vol": None,
        "net_gex_oi": None,
        "major_pos_vol": None,
        "major_neg_vol": None,
        "major_pos_oi": None,
        "major_neg_oi": None,
        "timestamp": None,
        "delta_risk_reversal": None,
        "strikes": None,
        "max_priors": None,
    }

    def _apply_values(source: dict) -> None:
        mapping = {
            "spot": "spot",
            "zero_gamma": "zero_gamma",
            "net_gex": "net_gex_vol",
            "net_gex_vol": "net_gex_vol",
            "sum_gex_vol": "net_gex_vol",
            "net_gex_oi": "net_gex_oi",
            "sum_gex_oi": "net_gex_oi",
            "major_pos_vol": "major_pos_vol",
            "major_neg_vol": "major_neg_vol",
            "major_pos_oi": "major_pos_oi",
            "major_neg_oi": "major_neg_oi",
        }
        for src_key, dest_key in mapping.items():
            if src_key in source and source[src_key] is not None:
                result[dest_key] = source[src_key]

        timestamp_value = source.get("timestamp")
        if isinstance(timestamp_value, (int, float)):
            result["timestamp"] = timestamp_value
        elif isinstance(timestamp_value, str):
            result["timestamp"] = timestamp_value

        if "delta_risk_reversal" in source:
            result["delta_risk_reversal"] = source.get("delta_risk_reversal")

        if "strikes" in source:
            result["strikes"] = source.get("strikes")

        if "max_priors" in source:
            result["max_priors"] = source.get("max_priors")

    if snapshot:
        _apply_values(snapshot)

    if majors:
        _apply_values(majors)

    parsed_maxchange: Dict[str, dict] = {}
    if maxchange:
        mapping = {
            "current": "Now",
            "one": "1 min",
            "five": "5 min",
            "ten": "10 min",
            "fifteen": "15 min",
            "thirty": "30 min",
        }
        for raw_key, label in mapping.items():
            value = maxchange.get(raw_key)
            if isinstance(value, (list, tuple)) and len(value) >= 2:
                parsed_maxchange[label] = {
                    "strike": value[0],
                    "net_gex": value[1],
                }

    result["max_change"] = parsed_maxchange
    return result


@dataclass
class SnapshotStore:
    """In-memory cache that persists the latest ticker snapshots to disk."""

    data: Dict[str, TickerSnapshot] = field(default_factory=dict)

    def update(
        self,
        ticker: str,
        kind: str,
        payload: dict,
        received_at: Optional[str],
        context: Optional[dict] = None,
    ) -> TickerSnapshot:
        """Update the snapshot for a ticker with new payload data."""
        canonical = normalise_ticker(ticker)
        if canonical not in self.data:
            self.data[canonical] = TickerSnapshot(ticker=canonical)
        snapshot = self.data[canonical]
        snapshot.merge_payload(kind, payload, received_at, context)
        LOG.debug("Updated %s (%s)", canonical, kind)
        return snapshot

    def to_dict(self) -> dict:
        """Convert all snapshots to a dictionary."""
        return {ticker: snapshot.to_dict() for ticker, snapshot in self.data.items()}

    def write_to_disk(self) -> None:
        """Write the current snapshots to disk as JSON."""
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        temp_file = OUTPUT_FILE.with_suffix(".json.tmp")
        with temp_file.open("w", encoding="utf-8") as handle:
            json.dump(self.to_dict(), handle, indent=2, sort_keys=True)
        os.replace(temp_file, OUTPUT_FILE)
        LOG.debug("Wrote %s", OUTPUT_FILE)


STORE = SnapshotStore()


class DepthPersistor:
    """Buffer depth events and flush them to disk (JSONL) in the background.

    Uses market_ml.depth.persist_raw_event to persist per-day JSONL files.
    Implements backpressure to prevent unbounded memory growth.
    """

    def __init__(self, flush_interval: float = 2.0, batch_size: int = 50, max_queue_size: int = 10000):
        """
        Initialize depth persistor with bounded queue.
        
        Args:
            flush_interval: Seconds between flushes
            batch_size: Number of events to batch
            max_queue_size: Maximum queue size (backpressure limit)
        """
        self.queue: Queue = Queue(maxsize=max_queue_size)
        self.flush_interval = float(flush_interval)
        self.batch_size = int(batch_size)
        self.max_queue_size = max_queue_size
        self._running = True
        self._thread = threading.Thread(target=self._run_loop, daemon=True, name="DepthPersistor")
        self._dropped_count = 0
        self._processed_count = 0
        self._error_count = 0
        self._retry_queue = RetryableQueue(max_retries=3, retry_delay=60.0)
        self._thread.start()
        LOG.info(f"DepthPersistor initialized with max_queue_size={max_queue_size}, batch_size={batch_size}")

    def add(self, event: dict) -> bool:
        """
        Add a depth event to the buffer queue.
        
        Returns:
            True if event was queued, False if dropped due to backpressure
        """
        try:
            self.queue.put(event, block=True, timeout=0.1)
            return True
        except:  # Queue full (includes Full exception)
            self._dropped_count += 1
            if self._dropped_count % 100 == 0:  # Log every 100th drop
                LOG.warning(
                    f"Depth queue backpressure: dropped {self._dropped_count} events "
                    f"(queue size: {self.queue.qsize()}/{self.max_queue_size})"
                )
            return False

    def _run_loop(self) -> None:
        """Main loop for processing queued events."""
        buffer = []
        last_flush = time.time()
        while self._running:
            try:
                # Try to get item with timeout
                item = self.queue.get(timeout=self.flush_interval)
                buffer.append(item)
                
                # Flush if buffer reached batch size
                if len(buffer) >= self.batch_size:
                    self._flush_buffer(buffer)
                    buffer = []
                    last_flush = time.time()
            except Empty:
                # Timed flush
                if buffer and (time.time() - last_flush) >= self.flush_interval:
                    self._flush_buffer(buffer)
                    buffer = []
                    last_flush = time.time()
                
                # Process retries
                self._process_retries()
                continue

        # Flush remaining on shutdown
        if buffer:
            LOG.info(f"Flushing {len(buffer)} remaining events on shutdown")
            self._flush_buffer(buffer)

    def _flush_buffer(self, buffer: list[dict]) -> None:
        """Flush buffered events to disk with retry on failure."""
        for event in buffer:
            try:
                persist_raw_event(event)
                self._processed_count += 1
            except Exception as e:
                self._error_count += 1
                LOG.error(f"Failed to persist depth event: {e}")
                # Add to retry queue
                self._retry_queue.add_failed_item(event, e)

    def _process_retries(self) -> None:
        """Process failed events that are ready for retry."""
        retryable = self._retry_queue.get_retryable_items()
        for failed_item in retryable:
            try:
                persist_raw_event(failed_item['item'])
                self._retry_queue.retry_item(failed_item, success=True)
                self._processed_count += 1
            except Exception as e:
                LOG.warning(f"Retry failed: {e}")
                self._retry_queue.retry_item(failed_item, success=False)

    def get_stats(self) -> dict:
        """Get persistor statistics."""
        retry_stats = self._retry_queue.get_stats()
        return {
            'queue_size': self.queue.qsize(),
            'max_queue_size': self.max_queue_size,
            'processed': self._processed_count,
            'dropped': self._dropped_count,
            'errors': self._error_count,
            'retry_queue': retry_stats
        }

    def shutdown(self, timeout: float = 5.0) -> None:
        """Shutdown the persistor thread."""
        LOG.info("Shutting down DepthPersistor...")
        stats = self.get_stats()
        LOG.info(f"Final stats: {stats}")
        self._running = False
        self._thread.join(timeout=timeout)


DEPTH_PERSISTOR = DepthPersistor()

# Ensure we flush on process exit
atexit.register(lambda: DEPTH_PERSISTOR.shutdown())


class GEXBridgeRequestHandler(BaseHTTPRequestHandler):
    """Minimal handler for POST /gex."""

    server_version = "GEXBridge/1.0"

    def do_OPTIONS(self) -> None:  # noqa: N802
        """Handle CORS preflight requests."""
        path = self.path.rstrip("/")
        if path not in {"/gex", "/gex_history_url", "/uw"}:
            self.send_error(HTTPStatus.NOT_FOUND, "Endpoint not found")
            return
        self.send_response(HTTPStatus.NO_CONTENT)
        self.send_header("Access-Control-Allow-Origin", "*")
        self.send_header("Access-Control-Allow-Methods", "POST, OPTIONS")
        self.send_header("Access-Control-Allow-Headers", "Content-Type")
        self.end_headers()

    def do_POST(self) -> None:  # noqa: N802 (BaseHTTPRequestHandler API)
        """Handle POST requests to /gex, /gex_history_url, and /uw."""
        path = self.path.rstrip("/")
        if path == "/gex_history_url":
            self._handle_history_url()
            return
        if path == "/uw":
            self._handle_universal_webhook()
            return
        if path != "/gex":
            self.send_error(HTTPStatus.NOT_FOUND, "Endpoint not found")
            return

        content_length = int(self.headers.get("Content-Length", "0"))
        if content_length <= 0:
            self.send_error(HTTPStatus.BAD_REQUEST, "Empty request body")
            return


        raw_body = self.rfile.read(content_length)
        LOG.debug("Raw POST body: %r", raw_body)
        try:
            payload = json.loads(raw_body.decode("utf-8"))
            LOG.debug("Parsed payload: %r", payload)
        except json.JSONDecodeError as exc:
            LOG.warning("Invalid JSON payload: %s", exc)
            self.send_error(HTTPStatus.BAD_REQUEST, "Invalid JSON payload")
            return
        if not isinstance(payload, dict):
            self.send_error(HTTPStatus.BAD_REQUEST, "Payload must be a JSON object")
            return

        # Validate GEX payload with Pydantic schema
        try:
            validated_payload = validate_gex_payload(payload)
        except ValueError as exc:
            LOG.warning("GEX payload validation failed: %s", exc)
            self.send_error(HTTPStatus.BAD_REQUEST, f"Validation error: {exc}")
            return
        
        # Extract validated fields
        data = validated_payload.data
        ticker = validated_payload.ticker
        received_at = validated_payload.received_at
        endpoint = validated_payload.endpoint
        payload_kind = validated_payload.kind

        try:
            context = {
                "endpoint": endpoint,
            }
            snapshot_obj = STORE.update(ticker, payload_kind, data, received_at, context)
            STORE.write_to_disk()
            
            # Write to new consolidated GEX structure (FR-001, FR-002)
            try:
                write_consolidated_gex_snapshot(snapshot_obj, data, payload_kind, endpoint)
            except Exception:
                LOG.exception("Failed to write consolidated GEX snapshot for %s", ticker)
            
            spot = data.get("spot")
            ts = data.get("timestamp")
            if spot is not None and ts is not None:
                append_spot_series(spot, ts)
            # Persist the raw payload to depth storage asynchronously (buffered)
            try:
                depth_event = dict(data)
                depth_event.update({
                    "timestamp": ts or datetime.utcnow().isoformat(),
                    "ticker": normalise_ticker(ticker),
                    "endpoint": endpoint or "",
                    "payload_kind": payload_kind,
                })
                DEPTH_PERSISTOR.add(depth_event)
            except Exception:
                LOG.exception("Failed to enqueue depth event for persistence")
            persist_snapshot_to_db(snapshot_obj.ticker, snapshot_obj, payload_kind, data, endpoint)
        except Exception as exc:  # pragma: no cover - safety net
            LOG.exception("Failed to store payload")
            self.send_error(HTTPStatus.INTERNAL_SERVER_ERROR, str(exc))
            return

        self.send_response(HTTPStatus.ACCEPTED)
        self.send_header("Content-Type", "application/json")
        self.send_header("Access-Control-Allow-Origin", "*")
        self.end_headers()
        response_body = json.dumps({"status": "ok", "ticker": normalise_ticker(ticker)})
        self.wfile.write(response_body.encode("utf-8"))

    def log_message(self, fmt: str, *args: object) -> None:  # noqa: D401
        """Route BaseHTTPRequestHandler log output through logging module."""
        LOG.info("%s - %s", self.address_string(), fmt % args)

    def _handle_history_url(self) -> None:
        """Handle POST requests to /gex_history_url for queuing history imports."""
        content_length = int(self.headers.get("Content-Length", "0"))
        if content_length <= 0:
            self.send_error(HTTPStatus.BAD_REQUEST, "Empty request body")
            return

        raw_body = self.rfile.read(content_length)
        LOG.debug("Raw history URL body: %r", raw_body)
        try:
            payload = json.loads(raw_body.decode("utf-8"))
        except json.JSONDecodeError as exc:
            LOG.warning("Invalid JSON payload for history queue: %s", exc)
            self.send_error(HTTPStatus.BAD_REQUEST, "Invalid JSON payload")
            return
        if not isinstance(payload, dict):
            self.send_error(HTTPStatus.BAD_REQUEST, "Payload must be a JSON object")
            return

        url = payload.get("url")
        ticker = payload.get("ticker")
        endpoint = payload.get("endpoint") or payload.get("feed") or payload.get("kind")

        if not url or not ticker or not endpoint:
            self.send_error(HTTPStatus.BAD_REQUEST, "Missing url, ticker, or endpoint")
            return

        # Try forwarding the history enqueue to the dashboard API first
        try:
            dashboard_url = os.getenv('DASHBOARD_URL', 'http://127.0.0.1:5801')
            ingest_url = f"{dashboard_url.rstrip('/')}/api/ingest/history"
            try:
                resp = requests.post(ingest_url, json={"url": url, "ticker": ticker, "endpoint": endpoint}, timeout=2)
                if resp.status_code in (200, 202):
                    self.send_response(HTTPStatus.ACCEPTED)
                    self.send_header("Content-Type", "application/json")
                    self.send_header("Access-Control-Allow-Origin", "*")
                    self.end_headers()
                    response_body = json.dumps({"status": "queued", "via": "dashboard"})
                    self.wfile.write(response_body.encode("utf-8"))
                    return
            except Exception:
                LOG.debug("Dashboard ingest/history not reachable, falling back to local enqueue")

            # Fallback to local enqueue if dashboard unavailable
            queue_id = enqueue_history_request(str(url), normalise_ticker(str(ticker)), str(endpoint))
        except Exception as exc:  # pragma: no cover
            LOG.exception("Failed to enqueue history URL")
            self.send_error(HTTPStatus.INTERNAL_SERVER_ERROR, str(exc))
            return

        self.send_response(HTTPStatus.ACCEPTED)
        self.send_header("Content-Type", "application/json")
        self.send_header("Access-Control-Allow-Origin", "*")
        self.end_headers()
        response_body = json.dumps({"status": "queued", "id": queue_id, "via": "local"})
        self.wfile.write(response_body.encode("utf-8"))

    def _handle_universal_webhook(self) -> None:
        content_length = int(self.headers.get("Content-Length", "0"))
        if content_length <= 0:
            self.send_error(HTTPStatus.BAD_REQUEST, "Empty request body")
            return

        raw_body = self.rfile.read(content_length)
        LOG.debug("Raw universal webhook body: %r", raw_body)
        try:
            payload = json.loads(raw_body.decode("utf-8"))
        except json.JSONDecodeError as exc:
            LOG.warning("Invalid JSON payload for universal webhook: %s", exc)
            self.send_error(HTTPStatus.BAD_REQUEST, "Invalid JSON payload")
            return
        if not isinstance(payload, dict):
            self.send_error(HTTPStatus.BAD_REQUEST, "Payload must be a JSON object")
            return

        # Validate universal webhook payload with Pydantic schema
        try:
            validated_payload = validate_webhook_payload(payload)
            # Check if this is a heartbeat that should be filtered
            if validated_payload is None:
                LOG.debug("Dropping filtered heartbeat message")
                self.send_response(HTTPStatus.OK)
                self.send_header("Content-Type", "application/json")
                self.send_header("Access-Control-Allow-Origin", "*")
                self.end_headers()
                response_body = json.dumps({"status": "ignored", "reason": "heartbeat"})
                self.wfile.write(response_body.encode("utf-8"))
                return
        except ValueError as exc:
            LOG.warning("Universal webhook validation failed: %s", exc)
            self.send_error(HTTPStatus.BAD_REQUEST, f"Validation error: {exc}")
            return

        # Drop noisy Phoenix heartbeat / ack messages (they're not actionable)
        try:
            topic = payload.get("topic")
            event_type = payload.get("eventType") or payload.get("event_type") or payload.get("event")
            inner = payload.get("payload") if isinstance(payload.get("payload"), dict) else {}
            # Drop trivial phoenix heartbeat/ack messages
            if topic == "phoenix" and event_type == "phx_reply" and inner.get("status") == "ok":
                LOG.debug("Dropping trivial phoenix phx_reply ok message")
                # Return 200 but do not persist or forward to Discord
                self.send_response(HTTPStatus.OK)
                self.send_header("Content-Type", "application/json")
                self.send_header("Access-Control-Allow-Origin", "*")
                self.end_headers()
                response_body = json.dumps({"status": "ignored", "reason": "heartbeat phoenix phx_reply"})
                self.wfile.write(response_body.encode("utf-8"))
                return

            # Handle GEX topic - process and persist GEX data
            if topic == "gex":
                LOG.debug("Processing GEX payload from universal webhook")
                try:
                    # Extract GEX data from validated payload
                    ticker = validated_payload.ticker
                    received_at = validated_payload.received_at
                    endpoint = validated_payload.endpoint or "gex_zero"
                    payload_kind = validated_payload.kind or "snapshot"
                    data = validated_payload.payload
                    
                    # Process GEX snapshot (same as /gex endpoint)
                    context = {"endpoint": endpoint}
                    snapshot_obj = STORE.update(ticker, payload_kind, data, received_at, context)
                    STORE.write_to_disk()
                    
                    # Write to consolidated GEX structure
                    try:
                        write_consolidated_gex_snapshot(snapshot_obj, data, payload_kind, endpoint)
                    except Exception:
                        LOG.exception("Failed to write consolidated GEX snapshot for %s", ticker)
                    
                    # Persist to database
                    persist_snapshot_to_db(snapshot_obj.ticker, snapshot_obj, payload_kind, data, endpoint)
                    
                    LOG.info("Successfully processed GEX data for %s (spot=%.2f, zero_gamma=%.2f)",
                            ticker, data.get('spot', 0.0), data.get('zero_gamma', 0.0))
                except Exception:
                    LOG.exception("Failed to process GEX payload for topic=gex")
            
            # Route messages intended for the other channel/topic to that channel
            if topic == "option_trades_super_algo":
                LOG.debug("Routing message for topic 'option_trades_super_algo' to alternate channel")
                # Send to alternate channel; channel id configurable via env var ALT_CHANNEL_OPTION_TRADES
                alt_channel = os.getenv('ALT_CHANNEL_OPTION_TRADES', '1425136266676146236')
                # Don't forward here (is_first not known yet); we'll route when composing the final send below
        except Exception:
            LOG.exception("Error while evaluating drop filter for universal webhook")

        # Log the universal webhook payload
        LOG.info("Received universal webhook: %s", json.dumps(payload, indent=2))

        # Determine if this is the first universal webhook we've received
        is_first = False
        try:
            if BRIDGE_DB_READY:
                with DB_LOCK, sqlite3.connect(BRIDGE_DB_PATH, timeout=30) as conn:
                    # Ensure table exists for counting
                    conn.execute(
                        """
                        CREATE TABLE IF NOT EXISTS universal_webhooks (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            received_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            payload_json TEXT NOT NULL,
                            source_ip TEXT,
                            user_agent TEXT
                        )
                        """
                    )
                    cur = conn.execute("SELECT COUNT(1) FROM universal_webhooks")
                    row = cur.fetchone()
                    if not row or row[0] == 0:
                        is_first = True
        except Exception:
            LOG.exception("Error checking universal_webhooks count")

        # Discord sending disabled - currently rate-limited
        # Data is stored in database for bot to process
        topic_val = payload.get('topic')
        # if topic_val == 'option_trades_super_algo':
        #     send_chan = os.getenv('ALT_CHANNEL_OPTION_TRADES')
        #     self._send_to_discord(payload, first=is_first, channel_id=send_chan)

        # Store in database if available (using a generic table or extending existing)
        try:
            if BRIDGE_DB_READY:
                with DB_LOCK, sqlite3.connect(BRIDGE_DB_PATH, timeout=30) as conn:
                    # Create universal webhooks table if it doesn't exist
                    conn.execute(
                        """
                        CREATE TABLE IF NOT EXISTS universal_webhooks (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            received_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            payload_json TEXT NOT NULL,
                            source_ip TEXT,
                            user_agent TEXT
                        )
                        """
                    )
                    # Store the payload
                    conn.execute(
                        """
                        INSERT INTO universal_webhooks (payload_json, source_ip, user_agent)
                        VALUES (?, ?, ?)
                        """,
                        (
                            json.dumps(payload),
                            self.address_string(),
                            self.headers.get("User-Agent", "")
                        )
                    )
                    conn.commit()
                    LOG.debug("Stored universal webhook payload in database")
                    # If this is an option_trades_super_algo payload, also persist a normalized row
                    try:
                        topic_val = payload.get('topic')
                        if topic_val == 'option_trades_super_algo':
                            # Ensure option_trades_events table exists
                            conn.execute(
                                """
                                CREATE TABLE IF NOT EXISTS option_trades_events (
                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                    received_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                                    topic TEXT,
                                    symbol TEXT,
                                    side TEXT,
                                    quantity INTEGER,
                                    price REAL,
                                    strike REAL,
                                    option_type TEXT,
                                    expiration TEXT,
                                    order_type TEXT,
                                    execution_status TEXT,
                                    broker TEXT,
                                    execution_qty INTEGER,
                                    execution_price REAL,
                                    raw_payload_json TEXT
                                )
                                """
                            )

                            # Extract common fields safely
                            alert = payload.get('alert') or {}
                            executions = payload.get('executions') or []
                            symbol = alert.get('symbol') or payload.get('symbol') or payload.get('ticker')
                            side = (alert.get('action') or payload.get('action') or payload.get('side') or '').upper()
                            qty = None
                            try:
                                qty = int(alert.get('quantity') or payload.get('quantity') or 0)
                            except Exception:
                                qty = None
                            price = None
                            try:
                                price = float(alert.get('price') or payload.get('price') or 0)
                            except Exception:
                                price = None
                            strike = None
                            try:
                                strike = float(alert.get('strike')) if alert.get('strike') is not None else None
                            except Exception:
                                strike = None
                            option_type = alert.get('option_type') or alert.get('optionType') or None
                            expiration = alert.get('expiration') or alert.get('exp') or None
                            order_type = alert.get('order_type') or alert.get('orderType') or None
                            exec_status = 'unknown'
                            broker = None
                            exec_qty = None
                            exec_price = None
                            if executions and isinstance(executions, (list, tuple)) and len(executions) > 0:
                                first_exec = executions[0]
                                exec_status = 'dry_run' if first_exec.get('dry_run') else 'live'
                                broker = first_exec.get('broker') or first_exec.get('venue')
                                try:
                                    exec_qty = int(first_exec.get('quantity') or 0)
                                except Exception:
                                    exec_qty = None
                                try:
                                    exec_price = float(first_exec.get('price') or first_exec.get('execution_price') or 0)
                                except Exception:
                                    exec_price = None

                            conn.execute(
                                """
                                INSERT INTO option_trades_events (
                                    topic, symbol, side, quantity, price, strike, option_type,
                                    expiration, order_type, execution_status, broker, execution_qty,
                                    execution_price, raw_payload_json
                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """,
                                (
                                    topic_val,
                                    str(symbol) if symbol is not None else None,
                                    side,
                                    qty,
                                    price,
                                    strike,
                                    option_type,
                                    expiration,
                                    order_type,
                                    exec_status,
                                    broker,
                                    exec_qty,
                                    exec_price,
                                    json.dumps(payload),
                                ),
                            )
                            conn.commit()
                            LOG.debug("Stored option_trades_super_algo normalized row")
                            # After persisting, send a short confirmation proposal to the alt channel
                            try:
                                # Compute a proposed half-quantity LMT for verification
                                proposed_qty = None
                                if qty is not None:
                                    try:
                                        proposed_qty = max(1, int(int(qty) // 2))
                                    except Exception:
                                        proposed_qty = None
                                limit_price = exec_price or price or 0.0
                                symbol_display = str(symbol) if symbol is not None else 'UNKNOWN'
                                side_display = side or 'UNKNOWN'
                                conf_msg = f" CONFIRM PROPOSAL  {symbol_display} {side_display} qty {qty} -> propose half {proposed_qty} LMT @ ${float(limit_price):.2f}"
                                try:
                                    alt_ch = os.getenv('ALT_CHANNEL_OPTION_TRADES') or None
                                    self._send_to_discord({'type': 'confirmation', 'message': conf_msg, 'raw': payload}, first=False, channel_id=alt_ch)
                                except Exception:
                                    LOG.exception('Failed to post confirmation proposal to Discord')
                            except Exception:
                                LOG.exception('Failed to prepare confirmation proposal')
                    except Exception:
                        LOG.exception("Failed to persist option_trades_super_algo normalized row")
        except Exception as exc:
            LOG.exception("Failed to store universal webhook payload")

        self.send_response(HTTPStatus.OK)
        self.send_header("Content-Type", "application/json")
        self.send_header("Access-Control-Allow-Origin", "*")
        self.end_headers()
        response_body = json.dumps({"status": "received", "message": "Universal webhook processed"})
        self.wfile.write(response_body.encode("utf-8"))

    def _send_to_discord(self, payload: dict, first: bool = False, channel_id: Optional[str] = None) -> None:
        """DISABLED - Previously sent messages to Discord. Now data is stored in database only.

        If `first` is True, apply special formatting to make the initial message stand out.
        """
        try:
            # Choose formatting for first vs subsequent messages
            if first:
                # Build a concise header + pretty JSON block for the first message
                header = " FIRST WEBHOOK RECEIVED  /uw (port 8877)"
                summary_lines = []
                # Try to extract a few common fields for a short summary
                if isinstance(payload, dict):
                    for key in ("type", "ticker", "symbol", "action", "username"):
                        if payload.get(key) is not None:
                            summary_lines.append(f"**{key}**: {payload.get(key)}")

                summary = "\n".join(summary_lines)
                pretty = json.dumps(payload, indent=2)
                message_content = f"{header}\n\n{summary}\n\n```json\n{pretty}\n```"
            else:
                message_content = self._format_discord_message(payload)

            # Send message to Discord via REST API (use bot token from env or discord-bot/.env)
            try:
                # Prefer token from process environment
                token = os.getenv('DISCORD_BOT_TOKEN')
                if not token:
                    # Fallback to discord-bot/.env
                    from pathlib import Path
                    env_path = Path('discord-bot') / '.env'
                    if env_path.exists():
                        for line in env_path.read_text().splitlines():
                            if line.strip().startswith('DISCORD_BOT_TOKEN='):
                                token = line.split('=', 1)[1].strip()
                                break

                # Allow caller to override the target channel (used for routing special topics)
                send_channel = channel_id or TARGET_CHANNEL_ID
                if not token:
                    LOG.warning('No DISCORD_BOT_TOKEN available; cannot POST to Discord via REST')
                elif not send_channel:
                    LOG.warning('No channel configured; cannot POST to Discord via REST')
                else:
                    import requests
                    url = f"https://discord.com/api/v10/channels/{send_channel}/messages"
                    headers = {'Authorization': f'Bot {token}', 'Content-Type': 'application/json'}
                    resp = requests.post(url, headers=headers, json={'content': message_content}, timeout=10)
                    try:
                        resp.raise_for_status()
                        LOG.info('Posted message to Discord via REST (status=%s, channel=%s)', resp.status_code, send_channel)
                    except Exception:
                        LOG.exception('Discord REST post failed: %s', getattr(resp, 'text', ''))

                    # Optionally DM an operator if configured
                    dm_user = os.getenv('DISCORD_DM_USER_ID')
                    if dm_user and token:
                        try:
                            # Create or open a DM channel
                            dm_url = 'https://discord.com/api/v10/users/@me/channels'
                            dm_resp = requests.post(dm_url, headers={'Authorization': f'Bot {token}', 'Content-Type': 'application/json'}, json={'recipient_id': str(dm_user)}, timeout=5)
                            dm_resp.raise_for_status()
                            dm_chan = dm_resp.json().get('id')
                            if dm_chan:
                                # Send a compact DM (avoid huge JSON blowups)
                                dm_msg = f"[Bridge] New webhook: {payload.get('topic') or payload.get('type') or 'webhook'}\nSummary: {payload.get('ticker') or payload.get('symbol') or ''}"
                                requests.post(f'https://discord.com/api/v10/channels/{dm_chan}/messages', headers={'Authorization': f'Bot {token}', 'Content-Type': 'application/json'}, json={'content': dm_msg}, timeout=5)
                                LOG.info('Sent DM to operator %s', dm_user)
                        except Exception:
                            LOG.exception('Failed to send DM to operator')

            except Exception:
                LOG.exception("Unexpected error while posting to Discord REST")

            LOG.info("Successfully sent chat message to Discord channel using bot; first=%s", first)

        except Exception as exc:
            LOG.exception("Failed to send chat message to Discord: %s", exc)

    def _format_discord_message(self, payload: dict) -> str:
        """DISABLED - Format the webhook payload (Discord integration disabled)."""
        try:
            # Check if this is a trade execution payload
            if payload.get("type") == "discord_trade_execution":
                alert = payload.get("alert", {})
                executions = payload.get("executions", [])
                username = payload.get("username", "Unknown")

                message = f" **Trade Executed by {username}**\n"
                message += f"**{alert.get('action', 'UNKNOWN')} {alert.get('symbol', 'UNKNOWN')}**"

                if alert.get('strike') and alert.get('option_type'):
                    message += f" ${alert.get('strike')}{alert.get('option_type')}"

                if alert.get('expiration'):
                    exp_date = alert.get('expiration')
                    if isinstance(exp_date, str) and 'T' in exp_date:
                        exp_date = exp_date.split('T')[0]
                    message += f" exp {exp_date}"

                message += f" @ ${alert.get('price', 0):.2f}\n\n"

                for execution in executions:
                    message += f" **{execution.get('broker', 'Unknown')}**: {execution.get('quantity', 0)} contracts"
                    if execution.get('dry_run'):
                        message += "  DRY RUN"
                    else:
                        message += "  LIVE ORDER"
                    message += "\n"

                return message

            # Generic webhook message
            else:
                message = " **Webhook Received**\n"
                message += f"```\n{json.dumps(payload, indent=2)}\n```"
                return message

        except Exception as exc:
            LOG.exception("Error formatting Discord message: %s", exc)
            return f" **Webhook Received**\n```\n{json.dumps(payload, indent=2)}\n```"


def parse_args() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Run local GEXBot bridge server")
    parser.add_argument("--host", default="127.0.0.1", help="Bind address (default: 127.0.0.1)")
    parser.add_argument("--port", type=int, default=8877, help="Port for incoming Tampermonkey POSTs")
    parser.add_argument("--debug", action="store_true", help="Enable verbose logging")
    parser.add_argument("--poll-interval", type=int, default=60, help="GEXBot API poll interval in seconds (default: 60)")
    return parser.parse_args()


def poll_gexbot_api(symbols: List[str], poll_interval: int = 60):
    """Background thread to poll GEXBot API and post to local webhook.
    
    Args:
        symbols: List of ticker symbols to poll (e.g., ['NQ_NDX', 'ES_SPX', 'SPY', 'QQQ'])
        poll_interval: Seconds between polls (default: 60)
    """
    api_key = config.GEXBOT_API_KEY
    if not api_key:
        LOG.warning("No GEXBOT_API_KEY configured - skipping API polling")
        return
    
    LOG.info("Starting GEXBot API polling for symbols: %s (interval: %ds)", symbols, poll_interval)
    
    def _poll_loop():
        while True:
            try:
                for symbol in symbols:
                    try:
                        # Fetch zero gamma data (current snapshot)
                        df_zero = load_historical_gex(
                            symbol=symbol,
                            start_date='2000-01-01',
                            end_date='2099-12-31',
                            aggregation='zero',
                            api_key=api_key
                        )
                        
                        if not df_zero.empty:
                            # Get latest snapshot
                            latest = df_zero.iloc[-1]
                            
                            # Create GEX payload matching webhook format
                            gex_payload = {
                                'topic': 'gex',
                                'ticker': symbol,
                                'endpoint': 'gex_zero',
                                'kind': 'snapshot',
                                'received_at': datetime.now().isoformat(),
                                'payload': {
                                    'ticker': symbol,
                                    'spot': float(latest.get('price', 0)),
                                    'zero_gamma': float(latest.get('zero_gamma', 0)),
                                    'net_gex_oi': float(latest.get('sum_gex_oi', 0)),
                                    'net_gex_vol': float(latest.get('sum_gex_vol', 0)),
                                    'major_pos_oi': float(latest.get('major_call_oi', 0)),
                                    'major_neg_oi': float(latest.get('major_put_oi', 0)),
                                    'major_pos_vol': float(latest.get('major_call_vol', 0)),
                                    'major_neg_vol': float(latest.get('major_put_vol', 0)),
                                    'timestamp': df_zero.index[-1].isoformat() if hasattr(df_zero.index[-1], 'isoformat') else str(df_zero.index[-1]),
                                }
                            }
                            
                            # Post to local /uw webhook (self-post)
                            try:
                                requests.post(
                                    'http://127.0.0.1:8877/uw',
                                    json=gex_payload,
                                    timeout=5
                                )
                                LOG.debug("Posted %s GEX zero data to local webhook", symbol)
                            except Exception as post_err:
                                LOG.error("Failed to post %s zero to webhook: %s", symbol, post_err)
                        
                        # Fetch maxchange data (temporal GEX changes) - direct API call
                        try:
                            import requests
                            maxchange_url = f"https://api.gexbot.com/{symbol}/classic/zero/maxchange"
                            maxchange_resp = requests.get(maxchange_url, params={'key': api_key}, timeout=10)
                            
                            if maxchange_resp.status_code == 200:
                                maxchange_data = maxchange_resp.json()
                                
                                # Create maxchange payload
                                maxchange_payload = {
                                    'topic': 'gex',
                                    'ticker': symbol,
                                    'endpoint': 'gex_maxchange',
                                    'kind': 'maxchange',
                                    'received_at': datetime.now().isoformat(),
                                    'payload': {
                                        'ticker': symbol,
                                        'timestamp': datetime.fromtimestamp(maxchange_data.get('timestamp', 0)).isoformat() if maxchange_data.get('timestamp') else datetime.now().isoformat(),
                                        # Maxchange format: [strike, gex_delta_millions]
                                        'current': maxchange_data.get('current', [0, 0]),
                                        'one': maxchange_data.get('one', [0, 0]),
                                        'five': maxchange_data.get('five', [0, 0]),
                                        'ten': maxchange_data.get('ten', [0, 0]),
                                        'fifteen': maxchange_data.get('fifteen', [0, 0]),
                                        'thirty': maxchange_data.get('thirty', [0, 0]),
                                    }
                                }
                                
                                # Post maxchange to webhook
                                try:
                                    requests.post(
                                        'http://127.0.0.1:8877/uw',
                                        json=maxchange_payload,
                                        timeout=5
                                    )
                                    LOG.debug("Posted %s GEX maxchange data to local webhook", symbol)
                                except Exception as post_err:
                                    LOG.error("Failed to post %s maxchange to webhook: %s", symbol, post_err)
                            else:
                                LOG.error("Maxchange API request failed for %s: %s %s", symbol, maxchange_resp.status_code, maxchange_resp.text)
                                    
                        except Exception as mc_err:
                            LOG.error("Error fetching maxchange for %s: %s", symbol, mc_err)
                                
                    except Exception as sym_err:
                        LOG.error("Error fetching GEX for %s: %s", symbol, sym_err)
                        
            except Exception as e:
                LOG.error("Error in GEXBot polling loop: %s", e)
                
            time.sleep(poll_interval)
    
    thread = threading.Thread(target=_poll_loop, daemon=True)
    thread.start()
    LOG.info("GEXBot API polling thread started")


def main() -> None:
    """Main entry point for the GEX bridge server."""
    args = parse_args()
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    server_address = (args.host, args.port)
    LOG.info("Starting GEX bridge on %s:%s", *server_address)

    # Start GEXBot API polling in background
    gex_symbols = ['NQ_NDX', 'ES_SPX', 'SPY', 'QQQ', 'SPX', 'NDX']
    poll_gexbot_api(gex_symbols, poll_interval=args.poll_interval)

    with ThreadingHTTPServer(server_address, GEXBridgeRequestHandler) as httpd:
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            LOG.info("Shutting down bridge server...")
            try:
                DEPTH_PERSISTOR.shutdown()
            except Exception:
                LOG.exception("Error shutting down DEPTH_PERSISTOR")


if __name__ == "__main__":
    main()
